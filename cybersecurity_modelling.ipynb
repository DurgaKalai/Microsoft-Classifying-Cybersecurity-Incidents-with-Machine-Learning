{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4031b00-851e-4779-867c-c2630583087e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbmNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\durga\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.4 MB 487.6 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/1.4 MB 465.5 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.4 MB 654.9 kB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.1/1.4 MB 653.6 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.2/1.4 MB 801.7 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.4 MB 744.2 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.4 MB 764.6 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.4 MB 849.3 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.4 MB 1.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.5/1.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24b01f2-2c82-4890-855a-8d0a41434398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05db95bd-078b-4900-9163-a1d8fba5c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the encoded train and test data\n",
    "# Load the data into memory (no mmap_mode)\n",
    "train_data = joblib.load('encoded_train_data1.joblib')\n",
    "test_data = joblib.load('encoded_test_data1.joblib')\n",
    "\n",
    "\n",
    "# Separateing features (X) and target variable (y)\n",
    "X = train_data.drop(columns=['IncidentGrade'])\n",
    "y = train_data['IncidentGrade']\n",
    "\n",
    "# Spliting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5328288-1826-4284-aae2-d47bb463dbcd",
   "metadata": {},
   "source": [
    "### evaluating best model for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1d34ee-e572-4a0d-93ac-c428e63bcb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.5616696298351134\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.87      0.66    416110\n",
      "           1       0.47      0.07      0.12    203156\n",
      "           2       0.69      0.47      0.56    332418\n",
      "\n",
      "    accuracy                           0.56    951684\n",
      "   macro avg       0.56      0.47      0.44    951684\n",
      "weighted avg       0.57      0.56      0.51    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[363085   8387  44638]\n",
      " [161550  13874  27732]\n",
      " [167529   7316 157573]]\n",
      "--------------------------------------------------\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9336323821772773\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94    416110\n",
      "           1       0.95      0.88      0.91    203156\n",
      "           2       0.96      0.92      0.94    332418\n",
      "\n",
      "    accuracy                           0.93    951684\n",
      "   macro avg       0.94      0.92      0.93    951684\n",
      "weighted avg       0.94      0.93      0.93    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[403873   5712   6525]\n",
      " [ 19692 178121   5343]\n",
      " [ 21663   4226 306529]]\n",
      "--------------------------------------------------\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.9555945040580697\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96    416110\n",
      "           1       0.93      0.93      0.93    203156\n",
      "           2       0.96      0.96      0.96    332418\n",
      "\n",
      "    accuracy                           0.96    951684\n",
      "   macro avg       0.95      0.95      0.95    951684\n",
      "weighted avg       0.96      0.96      0.96    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[400544   8183   7383]\n",
      " [  7915 189530   5711]\n",
      " [  7151   5917 319350]]\n",
      "--------------------------------------------------\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.7980653242042527\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.96      0.82    416110\n",
      "           1       0.88      0.57      0.69    203156\n",
      "           2       0.93      0.74      0.82    332418\n",
      "\n",
      "    accuracy                           0.80    951684\n",
      "   macro avg       0.84      0.76      0.78    951684\n",
      "weighted avg       0.83      0.80      0.79    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[398091   9114   8905]\n",
      " [ 76773 116193  10190]\n",
      " [ 81108   6088 245222]]\n",
      "--------------------------------------------------\n",
      "Model: XGBoost\n",
      "Accuracy: 0.9153637131652944\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92    416110\n",
      "           1       0.93      0.85      0.89    203156\n",
      "           2       0.94      0.90      0.92    332418\n",
      "\n",
      "    accuracy                           0.92    951684\n",
      "   macro avg       0.92      0.90      0.91    951684\n",
      "weighted avg       0.92      0.92      0.92    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[399091   7365   9654]\n",
      " [ 23503 171830   7823]\n",
      " [ 26885   5317 300216]]\n",
      "--------------------------------------------------\n",
      "Model: LightGBM\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2190\n",
      "[LightGBM] [Info] Number of data points in the train set: 380673, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -0.828430\n",
      "[LightGBM] [Info] Start training from score -1.541166\n",
      "[LightGBM] [Info] Start training from score -1.052299\n",
      "Accuracy: 0.8910100411481122\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90    416110\n",
      "           1       0.93      0.78      0.85    203156\n",
      "           2       0.94      0.87      0.91    332418\n",
      "\n",
      "    accuracy                           0.89    951684\n",
      "   macro avg       0.91      0.87      0.88    951684\n",
      "weighted avg       0.90      0.89      0.89    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[399148   6992   9970]\n",
      " [ 36138 158564   8454]\n",
      " [ 37383   4787 290248]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train_subsample = X_train.sample(frac=0.1, random_state=42)\n",
    "y_train_subsample = y_train.loc[X_train_subsample.index]\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_jobs=-1, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(n_jobs=-1, random_state=42),\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'Model: {model_name}')\n",
    "    \n",
    "    model.fit(X_train_subsample, y_train_subsample)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Evaluateing the models\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    report = classification_report(y_val, y_pred)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    # Displaying the results of the modles\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('Classification Report:')\n",
    "    print(report)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9e7805-9dbe-4ab7-b156-b826098ebbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Table:\n",
      "              Model  Accuracy  Macro-F1 Score  Precision  Recall\n",
      "Logistic Regression      0.56            0.44       0.56    0.47\n",
      "      Decision Tree      0.93            0.93       0.94    0.92\n",
      "      Random Forest      0.96            0.95       0.93    0.95\n",
      "            XGBoost      0.92            0.91       0.92    0.90\n",
      "           LightGBM      0.89            0.88       0.91    0.87\n",
      "  Gradient Boosting      0.80            0.78       0.84    0.76\n",
      "\n",
      "Best Model Based on Macro-F1 Score (and Accuracy in case of a tie):\n",
      "Model             Random Forest\n",
      "Accuracy                   0.96\n",
      "Macro-F1 Score             0.95\n",
      "Precision                  0.93\n",
      "Recall                     0.95\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Createing a report data\n",
    "report = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting'],\n",
    "    'Accuracy': [0.56,0.93,0.96,0.92,0.89,0.8],\n",
    "    'Macro-F1 Score': [0.44,0.93,0.95,0.91,0.88,0.78],\n",
    "    'Precision': [0.56,0.94,0.93,0.92,0.91,0.84],\n",
    "    'Recall': [0.47,0.92,0.95,0.90,0.87,0.76]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(report)\n",
    "\n",
    "print(\"Comparison Table:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "best_models_with_max_f1 = df[df['Macro-F1 Score'] == df['Macro-F1 Score'].max()]\n",
    "\n",
    "if len(best_models_with_max_f1) > 1:\n",
    "    best_model = best_models_with_max_f1.loc[best_models_with_max_f1['Accuracy'].idxmax()]\n",
    "else:\n",
    "    best_model = df.loc[df['Macro-F1 Score'].idxmax()]\n",
    "\n",
    "print(\"\\nBest Model Based on Macro-F1 Score (and Accuracy in case of a tie):\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f52136-7d4b-47b9-9e64-b2a56f672194",
   "metadata": {},
   "source": [
    "### Applying SMOTE to the training data for class imbalance and doing hyperparameter tuning for best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da325a3-b319-410d-b6d7-3d8ce5557045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Hyperparameters: {'n_estimators': 75, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90    416110\n",
      "           1       0.87      0.84      0.85    203156\n",
      "           2       0.93      0.88      0.91    332418\n",
      "\n",
      "    accuracy                           0.89    951684\n",
      "   macro avg       0.89      0.88      0.89    951684\n",
      "weighted avg       0.90      0.89      0.89    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[387516  15783  12811]\n",
      " [ 24330 170452   8374]\n",
      " [ 29291  10034 293093]]\n",
      "Model saved as rf_smote_tuned_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Loading the encoded train data\n",
    "train_data = joblib.load('encoded_train_data1.joblib')\n",
    "\n",
    "# Separating the features (X) and target variable (y)\n",
    "X = train_data.drop('IncidentGrade', axis=1)\n",
    "y = train_data['IncidentGrade']\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "# Splitting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Downsampling the training data to 2% for quicker processing\n",
    "X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.02, stratify=y_train, random_state=42)\n",
    "\n",
    "if X_train_sampled.select_dtypes(include=['bool']).shape[1] > 0:\n",
    "    X_train_sampled = X_train_sampled.astype(int)\n",
    "\n",
    "# Applying SMOTE for multi-class classification (default strategy balances all classes equally)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Hyperparameters for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 75],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=5,\n",
    "                                   cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fitting the Randomized Search with resampled training data\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluating on validation data\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Saving the tuned model\n",
    "joblib.dump(best_rf, \"rf_smote_tuned_model.joblib\")\n",
    "print(\"Model saved as rf_smote_tuned_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bebfd745-d39d-42a7-b104-c0f377fd657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Without SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90    416110\n",
      "           1       0.93      0.80      0.86    203156\n",
      "           2       0.94      0.87      0.91    332418\n",
      "\n",
      "    accuracy                           0.89    951684\n",
      "   macro avg       0.91      0.88      0.89    951684\n",
      "weighted avg       0.90      0.89      0.89    951684\n",
      "\n",
      "Confusion Matrix Without SMOTE:\n",
      "[[398387   7472  10251]\n",
      " [ 33405 161865   7886]\n",
      " [ 36507   5527 290384]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rf_no_smote_model.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the encoded train data\n",
    "train_data = joblib.load('encoded_train_data1.joblib')\n",
    "\n",
    "# Separating the features (X) and target variable (y)\n",
    "X = train_data.drop('IncidentGrade', axis=1)\n",
    "y = train_data['IncidentGrade']\n",
    "\n",
    "# Splitting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Downsampling the training data to 2% for quicker processing\n",
    "X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.02, stratify=y_train, random_state=42)\n",
    "\n",
    "# Random Forest without SMOTE\n",
    "rf_no_smote = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Training the model\n",
    "rf_no_smote.fit(X_train_sampled, y_train_sampled)\n",
    "y_pred_no_smote = rf_no_smote.predict(X_val)\n",
    "\n",
    "print(\"Classification Report Without SMOTE:\")\n",
    "print(classification_report(y_val, y_pred_no_smote))\n",
    "\n",
    "print(\"Confusion Matrix Without SMOTE:\")\n",
    "print(confusion_matrix(y_val, y_pred_no_smote))\n",
    "\n",
    "import joblib\n",
    "joblib.dump(rf_no_smote, \"rf_no_smote_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bddc5-26c9-4955-8270-b9ca28a7b751",
   "metadata": {},
   "source": [
    "### Evaluation of Best Random Forest Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cbb1704-e09c-4440-8a1f-bcc611c66918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.82   1630942\n",
      "           1       0.66      0.88      0.75    868897\n",
      "           2       0.89      0.87      0.88   1422856\n",
      "\n",
      "    accuracy                           0.83   3922695\n",
      "   macro avg       0.82      0.84      0.82   3922695\n",
      "weighted avg       0.84      0.83      0.83   3922695\n",
      "\n",
      "\n",
      "Macro-F1 Score: 0.82\n",
      "Macro Precision: 0.82\n",
      "Macro Recall: 0.84\n",
      "\n",
      "Confusion Matrix on Test Data:\n",
      "[[1238955  285289  106698]\n",
      " [  65844  761594   41459]\n",
      " [  69744  109961 1243151]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved Random Forest model\n",
    "best_rf = joblib.load(\"rf_smote_tuned_model.joblib\")\n",
    "\n",
    "# Loading the test dataset\n",
    "test_data = joblib.load('encoded_test_data1.joblib')\n",
    "\n",
    "# Separateing the features and target from test data\n",
    "X_test = test_data.drop('IncidentGrade', axis=1)  \n",
    "y_test = test_data['IncidentGrade']\n",
    "\n",
    "# Makeing predictions on the test data\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluateing the saved model on the test data\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "macro_f1 = report['macro avg']['f1-score']\n",
    "macro_precision = report['macro avg']['precision']\n",
    "macro_recall = report['macro avg']['recall']\n",
    "\n",
    "print(\"\\nMacro-F1 Score: {:.2f}\".format(macro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"\\nConfusion Matrix on Test Data:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a04a9-eca0-498a-a1ea-2a91dd5772af",
   "metadata": {},
   "source": [
    "### Applying SMOTE-ENN to the training data for class imbalance and doing hyperparameter tuning for best result\n",
    "(SMOTE + Edited Nearest Neighbors)\n",
    "#### SMOTE: Adds synthetic samples to balance the classes.\n",
    "#### SMOTE-ENN: Adds synthetic samples and then removes noisy or ambiguous samples for better data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e91174c-4568-491f-b986-6cd71f4c550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best Hyperparameters: {'n_estimators': 50, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': None, 'bootstrap': False}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84    416110\n",
      "           1       0.70      0.81      0.76    203156\n",
      "           2       0.94      0.76      0.84    332418\n",
      "\n",
      "    accuracy                           0.82    951684\n",
      "   macro avg       0.82      0.81      0.81    951684\n",
      "weighted avg       0.83      0.82      0.82    951684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[361764  43289  11057]\n",
      " [ 31838 165559   5759]\n",
      " [ 54794  26006 251618]]\n",
      "Model saved as rf_smote_enn_tuned_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Loading the encoded train data\n",
    "train_data = joblib.load('encoded_train_data1.joblib')\n",
    "\n",
    "# Separating the features (X) and target variable (y)\n",
    "X = train_data.drop('IncidentGrade', axis=1)\n",
    "y = train_data['IncidentGrade']\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna(axis=1)\n",
    "\n",
    "# Splitting the data (80:20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Downsampling the training data to 2% for quicker processing\n",
    "X_train_sampled, _, y_train_sampled, _ = train_test_split(X_train, y_train, train_size=0.02, stratify=y_train, random_state=42)\n",
    "\n",
    "if X_train_sampled.select_dtypes(include=['bool']).shape[1] > 0:\n",
    "    X_train_sampled = X_train_sampled.astype(int)\n",
    "\n",
    "# Applying SMOTE for multi-class classification (default strategy balances all classes equally)\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Hyperparameters for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 75],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=5,\n",
    "                                   cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fitting the Randomized Search with resampled training data\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters and model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Evaluating on validation data\n",
    "y_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# Saving the tuned model\n",
    "joblib.dump(best_rf, \"rf_smote_enn_tuned_model.joblib\")\n",
    "print(\"Model saved as rf_smote_enn_tuned_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3c39f0a-0eb8-4dd0-a479-262888edf64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.82   1630942\n",
      "           1       0.66      0.88      0.75    868897\n",
      "           2       0.89      0.87      0.88   1422856\n",
      "\n",
      "    accuracy                           0.83   3922695\n",
      "   macro avg       0.82      0.84      0.82   3922695\n",
      "weighted avg       0.84      0.83      0.83   3922695\n",
      "\n",
      "\n",
      "Macro-F1 Score: 0.82\n",
      "Macro Precision: 0.82\n",
      "Macro Recall: 0.84\n",
      "\n",
      "Confusion Matrix on Test Data:\n",
      "[[1238950  285295  106697]\n",
      " [  65849  761590   41458]\n",
      " [  69747  109959 1243150]]\n"
     ]
    }
   ],
   "source": [
    "#finally predicting on test data using \n",
    "# Loading the saved Random Forest model\n",
    "best_rf = joblib.load(\"rf_smote_tuned_model.joblib\")\n",
    "\n",
    "# Loading the test dataset\n",
    "test_data = joblib.load('encoded_test_data1.joblib')\n",
    "\n",
    "# Separateing the features and target from test data\n",
    "X_test = test_data.drop('IncidentGrade', axis=1)  \n",
    "y_test = test_data['IncidentGrade']\n",
    "\n",
    "# Makeing predictions on the test data\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluateing the saved model on the test data\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "macro_f1 = report['macro avg']['f1-score']\n",
    "macro_precision = report['macro avg']['precision']\n",
    "macro_recall = report['macro avg']['recall']\n",
    "\n",
    "print(\"\\nMacro-F1 Score: {:.2f}\".format(macro_f1))\n",
    "print(\"Macro Precision: {:.2f}\".format(macro_precision))\n",
    "print(\"Macro Recall: {:.2f}\".format(macro_recall))\n",
    "print(\"\\nConfusion Matrix on Test Data:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb674ac-00d5-425c-93b6-557e282e9ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
